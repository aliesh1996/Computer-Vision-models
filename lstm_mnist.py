# -*- coding: utf-8 -*-
"""LSTM_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17DAsJJAmJhV-keHVrFUAO0WhRcLIkARl
"""

import torch
import numpy as np
from matplotlib import pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as dsets

# determining the device
device = torch.device("cuda")
print(device)

# importing MNIST dataset
train_dataset = dsets.MNIST(root='./data',train=True,transform=transforms.ToTensor(),download=True)
test_dataset = dsets.MNIST(root='./data',train=False,transform=transforms.ToTensor(),download=True)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=100,shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=10000,shuffle=False)

# defining a function for one_hot
def one_hot(x):
  y=torch.zeros(x.shape[0],x.shape[1])
  for i in range(x.shape[0]):
    y[i,torch.argmax(x[i])]=1
  y.requires_grad_()
  y=y.to(device)
  return y

# defining the model class
class Model(nn.Module):
  def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):
    super(Model,self).__init__()
    self.hidden_dim = hidden_dim
    self.layer_dim = layer_dim
    self.lstm=nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)
    self.fc=nn.Linear(hidden_dim, output_dim)
  def forward(self,x):
    h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()
    c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()
    h0=h0.to(device)
    c0=c0.to(device)
    output, (h, c) = self.lstm(x, (h0.detach(), c0.detach()))
    # output =one_hot(self.fc(output[:, -1, :]))
    output =F.softmax(self.fc(output[:, -1, :]))
    return output

# training the model for 32 hidden layers
lossfunc_train=np.zeros((150,1))
lossfunc_test=np.zeros((150,1))
accuracy_test=np.zeros((150,1))
model1=Model(28,32,1,10)
model1.to(device)
optimizer=optim.Adam(model1.parameters(), lr=0.001)
criterion=nn.CrossEntropyLoss()

epoch=50
counter=0
for i in range(epoch):
  running_loss = 0.0
  for j,data in enumerate(train_loader,0):
    xtrain,ytrain=data[0].to(device), data[1].to(device)
    xtrain=xtrain.view(-1, 28, 28)
    optimizer.zero_grad()
    output=model1(xtrain)
    loss=criterion(output,ytrain)
    loss.backward()
    optimizer.step()
    running_loss += loss.item()
    if j % 200 == 199:    # print every 200 mini-batches
      print('[%d, %5d] loss: %.3f' %(i + 1, j + 1, running_loss / 200))
      lossfunc_train[counter]=running_loss/200
      running_loss = 0.0
      a=0
      for data in test_loader:
        xtest,ytest=data[0].to(device), data[1].to(device)
        xtest=xtest.view(-1, 28, 28)
        output_test=model1(xtest)
        loss_test=criterion(output_test,ytest)
        lossfunc_test[counter]=loss_test.item()
        print(loss_test.item())
        for p in range(ytest.shape[0]):
          if torch.argmax(output_test[p])==ytest[p]:
              a+=1
        accuracy_test[counter]=(a/10000)*100
        print('accuracy_test: %.3f'%((a/10000)*100))
      counter+=1

hminibatch=np.arange(1,151)
minibatch=hminibatch.reshape(150,1)

#plot the loss
plt.title("Loss Function(32)")
plt.xlabel('mini-batches per200')
plt.ylabel('Loss')
plt.plot(hminibatch, lossfunc_train)
plt.plot(hminibatch, lossfunc_test)
plt.show()
#plot the accuracy
plt.title("Accuracy(32)")
plt.xlabel('mini-batches per200')
plt.ylabel('Accuracy(test)')
plt.plot(hminibatch, accuracy_test)
plt.show()

# training the model for 64 hidden layers
lossfunc_train=np.zeros((150,1))
lossfunc_test=np.zeros((150,1))
accuracy_test=np.zeros((150,1))
model1=Model(28,64,1,10)
model1.to(device)
optimizer=optim.Adam(model1.parameters(), lr=0.001)
criterion=nn.CrossEntropyLoss()

epoch=50
counter=0
for i in range(epoch):
  running_loss = 0.0
  for j,data in enumerate(train_loader,0):
    xtrain,ytrain=data[0].to(device), data[1].to(device)
    xtrain=xtrain.view(-1, 28, 28)
    optimizer.zero_grad()
    output=model1(xtrain)
    loss=criterion(output,ytrain)
    loss.backward()
    optimizer.step()
    running_loss += loss.item()
    if j % 200 == 199:    # print every 200 mini-batches
      print('[%d, %5d] loss: %.3f' %(i + 1, j + 1, running_loss / 200))
      lossfunc_train[counter]=running_loss/200
      running_loss = 0.0
      a=0
      for data in test_loader:
        xtest,ytest=data[0].to(device), data[1].to(device)
        xtest=xtest.view(-1, 28, 28)
        output_test=model1(xtest)
        loss_test=criterion(output_test,ytest)
        lossfunc_test[counter]=loss_test.item()
        print(loss_test.item())
        for p in range(ytest.shape[0]):
          if torch.argmax(output_test[p])==ytest[p]:
              a+=1
        accuracy_test[counter]=(a/10000)*100
        print('accuracy_test: %.3f'%((a/10000)*100))
      counter+=1

hminibatch=np.arange(1,151)
minibatch=hminibatch.reshape(150,1)

#plot the loss
plt.title("Loss Function(64)")
plt.xlabel('mini-batches per200')
plt.ylabel('Loss')
plt.plot(hminibatch, lossfunc_train)
plt.plot(hminibatch, lossfunc_test)
plt.show()
#plot the accuracy
plt.title("Accuracy(64)")
plt.xlabel('mini-batches per200')
plt.ylabel('Accuracy(test)')
plt.plot(hminibatch, accuracy_test)
plt.show()

# training the model for 128 hidden layers
lossfunc_train=np.zeros((150,1))
lossfunc_test=np.zeros((150,1))
accuracy_test=np.zeros((150,1))
model2=Model(28,128,1,10)
model2.to(device)
optimizer=optim.Adam(model2.parameters(), lr=0.001)
criterion=nn.CrossEntropyLoss()
epoch=50
counter=0
for i in range(epoch):
  running_loss = 0.0
  for j,data in enumerate(train_loader,0):
    xtrain,ytrain=data[0].to(device), data[1].to(device)
    xtrain=xtrain.view(-1, 28, 28)
    optimizer.zero_grad()
    output=model2(xtrain)
    loss=criterion(output,ytrain)
    loss.backward()
    optimizer.step()
    running_loss += loss.item()
    if j % 200 == 199:    # print every 200 mini-batches
      print('[%d, %5d] loss: %.3f' %(i + 1, j + 1, running_loss / 200))
      lossfunc_train[counter]=running_loss/200
      running_loss = 0.0
      a=0
      for data in test_loader:
        xtest,ytest=data[0].to(device), data[1].to(device)
        xtest=xtest.view(-1, 28, 28)
        output_test=model2(xtest)
        loss_test=criterion(output_test,ytest)
        lossfunc_test[counter]=loss_test.item()
        for p in range(ytest.shape[0]):
          if torch.argmax(output_test[p])==ytest[p]:
              a+=1
        accuracy_test[counter]=(a/10000)*100
        print('accuracy_test: %.3f'%((a/10000)*100))
      counter+=1

hminibatch=np.arange(1,151)
minibatch=hminibatch.reshape(150,1)

#plot the loss
plt.title("Loss Function(128)")
plt.xlabel('mini-batches per200')
plt.ylabel('Loss')
plt.plot(hminibatch, lossfunc_train)
plt.plot(hminibatch, lossfunc_test)
plt.show()
#plot the accuracy
plt.title("Accuracy(128)")
plt.xlabel('mini-batches per200')
plt.ylabel('Accuracy(test)')
plt.plot(hminibatch, accuracy_test)
plt.show()